{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNivFS0IGR07nIIadWvwFRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyutikandala/ME36_Miniproject1/blob/main/ME36_Mini_Project1_Dyutikandala_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ME36 - Mini Project 1 - A Comprehensive ETL Workflow with Python for Data Engineers.\n",
        "\n",
        "# Problem statement: This project highlights the practical implementation of ETL processes using Python.\n",
        "# This involves data extraction from multiple file formats, transformation of units, and loading of the final data into a structured CSV format.\n",
        "# Demonstrate essential data engineering skills. Additionally, by logging each step of the process you can monitor the progress and debug issues if they arise.\n",
        "\n",
        "# Importing required libraries.\n",
        "\n",
        "import glob                         # this helps to handle different file formats.\n",
        "import pandas as pd                 # this helps to read CSV and JSON files.\n",
        "import xml.etree.ElementTree as ET  # this helps to parse XML data.\n",
        "from datetime import datetime       # this helps to track the progress of each phase through logging.\n",
        "import os\n",
        "\n",
        "# Setting up file names to store logs and transformed data in current working directory.\n",
        "\n",
        "timestamp_suffix = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "logfile    = \"logfile_\"+timestamp_suffix+\".txt\"            # All ETL event logs will be recorded into this file.\n",
        "targetfile = \"transformed_data.csv\"   # Target file to store transformed data.\n",
        "\n",
        "#Removing source files from previous run, if any\n",
        "for f in glob.glob('source*'):\n",
        "  os.remove(f)\n",
        "\n",
        "#Remove target file from previous run, if any.\n",
        "if os.path.exists(targetfile):\n",
        "  os.remove(targetfile)\n",
        "\n",
        "# Downloading the source dataset in .zip, containing multiple file formats from given URL into current working directory.\n",
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0221EN-SkillsNetwork/labs/module%206/Lab%20-%20Extract%20Transform%20Load/data/source.zip\n",
        "\n",
        "# Unzipping all source files into current working directory.\n",
        "!unzip source.zip\n",
        "\n",
        "# ****************************************** EXTRACT PHASE ********************************************************\n",
        "\n",
        "# Extract Approach:Three different functions to extract data from CSV, JSON, and XML files respectively.\n",
        "#                  Then a master function to combine all files into one single data frame.\n",
        "\n",
        "# CSV Extract Function\n",
        "def extract_from_csv(file_to_process):\n",
        "    dataframe = pd.read_csv(file_to_process)\n",
        "    return dataframe\n",
        "\n",
        "# JSON Extract Function\n",
        "def extract_from_json(file_to_process):\n",
        "    dataframe = pd.read_json(file_to_process,lines=True)\n",
        "    return dataframe\n",
        "\n",
        "# XML Extract Function\n",
        "def extract_from_xml(file_to_process):\n",
        "    #dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"])\n",
        "    tree = ET.parse(file_to_process)\n",
        "    root = tree.getroot()\n",
        "    rows=[]\n",
        "    for person in root:                               # Looping for each element i.e. each person in xml.\n",
        "        name = person.find(\"name\").text               # Storing value from name field for the iteration.\n",
        "        height = float(person.find(\"height\").text)    # Storing value from height field value for the iteration.\n",
        "        weight = float(person.find(\"weight\").text)    # Storing value from weight field value for the iteration.\n",
        "\n",
        "        #dataframe = pd.concat([dataframe,pd.DataFrame({\"name\":name, \"height\":height, \"weight\":weight},ignore_index=True)],ignore_index=True)\n",
        "        rows.append({\"name\":name, \"height\":height, \"weight\":weight}) # storing data into the master dataframe for the iteration\n",
        "\n",
        "    dataframe = pd.DataFrame(rows,columns=[\"name\", \"height\", \"weight\"])\n",
        "    return dataframe\n",
        "\n",
        "# A master function to combine the extracted data from multiple files into a single DataFrame.\n",
        "def extract():\n",
        "    # creating an empty data frame to hold extracted data.\n",
        "    extracted_data = pd.DataFrame(columns=['name','height','weight'])\n",
        "\n",
        "    # Finding CSV files and writing data into the above created dataframe.\n",
        "    for csvfile in glob.glob(\"*.csv\"):    # finding .csv files in current directory\n",
        "        extracted_data = pd.concat([extracted_data,extract_from_csv(csvfile)], ignore_index=True)\n",
        "\n",
        "    # Finding JSON files and writing data into the above created dataframe.\n",
        "    for jsonfile in glob.glob(\"*.json\"):   # finding .json files in current directory\n",
        "        extracted_data = pd.concat([extracted_data,extract_from_json(jsonfile)], ignore_index=True)\n",
        "\n",
        "    # Finding  XML files and writing data into the above created dataframe.\n",
        "    for xmlfile in glob.glob(\"*.xml\"):     # finding . xml files in current directory\n",
        "        extracted_data = pd.concat([extracted_data,extract_from_xml(xmlfile)], ignore_index=True)\n",
        "\n",
        "    # Data cleansing i.e. removing duplicate data if required.\n",
        "    # extracted_data = extracted_data.drop_duplicates()\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "# ****************************************** TRANSFORM PHASE ********************************************************\n",
        "\n",
        "# Transformations required: Convert height from inches to meters and weight from pounds to kilograms.\n",
        "# Transform approach: A function to perform arthemetic operations convert height and weight fields as per appropriate converstion formule.\n",
        "#                     and replacing the old value with converted value in the dataframe.\n",
        "\n",
        "def transform(data):\n",
        "\n",
        "        #Converting height from inches to meters and rounding off to two decimals(Conversion formula: one inch is 0.0254 meters)\n",
        "        data['height'] = round(data.height * 0.0254,2)\n",
        "\n",
        "        #Converting  weight from pounds to kilograms and round off to two decimals(Conversion formula: one pound is 0.45359237 kilograms)\n",
        "        data['weight'] = round(data.weight * 0.45359237,2)\n",
        "\n",
        "        return data  # Returning dataframe with converted data.\n",
        "\n",
        "# ***************************************  LOAD PHASE *************************************************************\n",
        "\n",
        "# Function for Loading extracted and transformed data set into a target .csv file, which can later be loaded into a relational database or used for further processing.\n",
        "\n",
        "def load(targetfile,data_to_load):\n",
        "\n",
        "    data_to_load.to_csv(targetfile) #Loading transformed data into Target file.\n",
        "\n",
        "# ************************************ LOGGING **************************************************************\n",
        "\n",
        "# A function to log each ETL phase (Extraction, Transformation, Loading) with a timestamp to ensure traceability and monitoring.\n",
        "# Logs are generated with timestamp and stage of each phase, written to text file for auditing or troubleshooting purposes.\n",
        "\n",
        "def log(message):\n",
        "\n",
        "    timestamp_format = '%Y-%h-%d %H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second\n",
        "    now = datetime.now() # get current timestamp\n",
        "    timestamp = now.strftime(timestamp_format) # Convert timestamp to string\n",
        "\n",
        "    with open(logfile,\"a\") as f:  # creates a log.txt file if None\n",
        "        f.write(timestamp + ',' + message + '\\n')\n",
        "\n",
        "\n",
        "# *********************************** EXECUTION ************************************************\n",
        "\n",
        "# The ETL process follows the following sequence - Extract phase -> Transform phase -> Load phase.\n",
        "# Logging happens at the start and end of each phase.\n",
        "\n",
        "log(\"ETL process Started.\")        # Calling log() function to log ETL job start.\n",
        "\n",
        "log(\"Extract phase Started.\")      # Calling log() function to log beginning of Extract phase.\n",
        "extracted_data = extract()         # Calling extract() master function, returns \"extracted_data\" from all source files in single dataframe.\n",
        "log(\"Extract phase Ended.\")        # Calling log() function to log ending of Extract phase.\n",
        "\n",
        "log(\"Transform phase Started.\")                 # Calling log() function to log beginning of Transform phase.\n",
        "transformed_data = transform(extracted_data)    # Calling transform() function with \"extracted_data\" dataframe as input. Returns the same dataframe with converted values for height and weight fields.\n",
        "log(\"Transform phase Ended.\")                   # Calling log() function to log ending of transform phase.\n",
        "\n",
        "log(\"Load phase Started.\")                      # Calling log() function to log beginning of load phase.\n",
        "load(targetfile,transformed_data)               # Calling load() function with transformed_data and .CSV target file as input.\n",
        "log(\"Load phase Ended.\")                        # Calling log() function to log ending of load phase.\n",
        "\n",
        "log(\"ETL Process Ended\")             # Calling log() function to log ETL job end.\n",
        "\n"
      ],
      "metadata": {
        "id": "1Ncvh9lgAnpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc253d2c-8434-41a5-8425-b8cee2882b04"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-31 13:43:52--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0221EN-SkillsNetwork/labs/module%206/Lab%20-%20Extract%20Transform%20Load/data/source.zip\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2707 (2.6K) [application/zip]\n",
            "Saving to: ‘source.zip’\n",
            "\n",
            "source.zip          100%[===================>]   2.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-31 13:43:52 (666 MB/s) - ‘source.zip’ saved [2707/2707]\n",
            "\n",
            "Archive:  source.zip\n",
            "  inflating: source3.json            \n",
            "  inflating: source1.csv             \n",
            "  inflating: source2.csv             \n",
            "  inflating: source3.csv             \n",
            "  inflating: source1.json            \n",
            "  inflating: source2.json            \n",
            "  inflating: source1.xml             \n",
            "  inflating: source2.xml             \n",
            "  inflating: source3.xml             \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-58f8e3882f56>:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  extracted_data = pd.concat([extracted_data,extract_from_csv(csvfile)], ignore_index=True)\n"
          ]
        }
      ]
    }
  ]
}